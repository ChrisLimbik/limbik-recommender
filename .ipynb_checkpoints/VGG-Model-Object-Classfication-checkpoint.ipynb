{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks are now capable of outperforming humans on some computer vision tasks, such as classifying images.\n",
    "\n",
    "That is, given a photograph of an object, answer the question as to which of 1,000 specific objects the photograph shows.\n",
    "\n",
    "A competition-winning model for this task is the VGG model by researchers at Oxford. What is important about this model, besides its capability of classifying objects in photographs, is that the model weights are freely available and can be loaded and used in your own models and applications.\n",
    "\n",
    "In this notebook, you will learn:\n",
    "* About the ImageNet dataset and competition and the VGG winning models.\n",
    "* How to load the VGG model in Keras and summarize its structure.\n",
    "* How to use the loaded VGG model to classifying objects in ad hoc photographs.\n",
    "\n",
    "Let’s get started.\n",
    "\n",
    "## ImageNet\n",
    "[ImageNet](http://www.image-net.org/) is a research project to develop a large database of images with annotations, e.g. images and their descriptions.\n",
    "\n",
    "The images and their annotations have been the basis for an image classification challenge called the [ImageNet Large Scale Visual Recognition Challenge](http://www.image-net.org/challenges/LSVRC/) or ILSVRC since 2010. The result is that research organizations battle it out on pre-defined datasets to see who has the best model for classifying the objects in images.\n",
    "\n",
    "For the classification task, images must be classified into one of 1,000 different categories.\n",
    "\n",
    "For the last few years very deep convolutional neural network models have been used to win these challenges and results on the tasks have exceeded human performance.\n",
    "\n",
    "## The Oxford VGG Models\n",
    "Researchers from the [Oxford Visual Geometry Group](http://www.robots.ox.ac.uk/~vgg/), or VGG for short, participate in the ILSVRC challenge.\n",
    "\n",
    "In 2014, convolutional neural network models (CNN) developed by the VGG [won the image classification tasks](http://image-net.org/challenges/LSVRC/2014/results). After the competition, the participants wrote up their findings in the paper: [\"Very Deep Convolutional Networks for Large-Scale Image Recognition\"](https://arxiv.org/abs/1409.1556).\n",
    "\n",
    "They also made their models and learned weights available online. This allowed other researchers and developers to use a state-of-the-art image classification model in their own work and programs.\n",
    "\n",
    "This helped to fuel a rash of transfer learning work where pre-trained models are used with minor modification on wholly new predictive modeling tasks, harnessing the state-of-the-art feature extraction capabilities of proven models.\n",
    "\n",
    "The VGG models are not longer state-of-the-art by only a few percentage points. Nevertheless, they are very powerful models and useful both as image classifiers and as the basis for new models that use image inputs.\n",
    "\n",
    "In the next section, we will see how we can use the VGG model directly in Keras.\n",
    "\n",
    "## Load the VGG Model in Keras\n",
    "The VGG model can be loaded and used in the Keras deep learning library.\n",
    "\n",
    "Keras provides an Applications interface for loading and using pre-trained models. Using this interface, we can create a VGG model using the pre-trained weights provided by the Oxford group and use it as a starting point in our own model, or use it as a model directly for classifying images.\n",
    "\n",
    "Keras provides both the 16-layer and 19-layer version via the VGG16 and VGG19 classes. Let’s focus on the VGG16 model.\n",
    "\n",
    "The model can be created as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 183s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "model = VGG16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the standard Keras tools for inspecting the model structure.\n",
    "\n",
    "For example, we can print a summary of the network layers as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is huge. We can also see that, by default, the model expects images as input with the size 224 x 224 pixels with 3 channels (e.g. color).\n",
    "\n",
    "The VGG() class takes a few arguments:\n",
    "* **include_top** (True): Whether or not to include the output layers for the model. We don’t need these if we are fitting the model on our own problem.\n",
    "* **weights** (‘imagenet‘): What weights to load. We can specify None to not load pre-trained weights if we are interested in training the model ourself from scratch.\n",
    "* **input_tensor** (None): A new input layer if we intend to fit the model on new data of a different size.\n",
    "* **input_shape** (None): The size of images that the model is expected to take if we change the input layer.\n",
    "* **pooling** (None): The type of pooling to use when we are training a new set of output layers.\n",
    "* **classes** (1000): The number of classes (e.g. size of output vector) for the model.\n",
    "\n",
    "Next, let’s look at using the loaded VGG model to classify ad hoc photographs.\n",
    "\n",
    "## Develop a Simple Photo Classifier\n",
    "Using the images from Google Drive provided by Limbik, I chose a few ones and loaded into the directory. For example, below is an image from Paul Rudd's video.\n",
    "\n",
    "![paul-rudd](test-images/Paul-Rudd.png)\n",
    "\n",
    "Since the VGG-16 model has been loaded, I now can load the image as pixel data and prepare it to be presented to the network.\n",
    "\n",
    "Keras provides some tools to help with this step.\n",
    "\n",
    "First, I use the *load_img()* function to load the image and resize it to the required size of 224×224 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "# load the Paul Rudd image\n",
    "paulrudd = load_img('test-images/Paul-Rudd.png', target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I can convert the pixels to a NumPy array so that I can work with it in Keras. I use the *img_to_array()* function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array\n",
    "# convert the image pixels to a numpy array\n",
    "paulrudd = img_to_array(paulrudd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network expects one or more images as input; that means the input array will need to be 4-dimensional: samples, rows, columns, and channels.\n",
    "\n",
    "I only have one sample (one image). I can reshape the array by calling *reshape()* and adding the extra dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape data for the model\n",
    "paulrudd = paulrudd.reshape((1, paulrudd.shape[0], paulrudd.shape[1], paulrudd.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the image pixels need to be prepared in the same way as the ImageNet training data was prepared. Keras provides a function called *preprocess_input()* to prepare new input for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input\n",
    "# prepare the image for the VGG model\n",
    "paulrudd = preprocess_input(paulrudd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am now ready to make a prediction for the Paul Rudd image. I call the *predict()* function on the model in order to get a prediction of the probability of the image belonging to each of the 1000 known object types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict the probability across all output classes\n",
    "prob = model.predict(paulrudd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly there, now I need to interpret the probabilities. \n",
    "\n",
    "Keras provides a function to interpret the probabilities called *decode_predictions().* It can return a list of classes and their probabilities in case I would like to present the top 3 objects that may be in the photo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\n",
      "40960/35363 [==================================] - 1s 22us/step\n",
      "Predicted: [('n06359193', 'web_site', 0.91855925), ('n03782006', 'monitor', 0.027165547), ('n04404412', 'television', 0.009196592)]\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import decode_predictions\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "print('Predicted:', decode_predictions(prob, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, I can see that the top 3 objects in the image are classified as: \n",
    "* \"website\" with a 91.9% likelihood.\n",
    "* \"monitor\" with a 2.7% likelihood.\n",
    "* \"television\" with a 0.9% likelihood.\n",
    "\n",
    "Okay, let's try on a couple more images. Here's one from the video with a dog named 'Tuggy':\n",
    "\n",
    "![tuggy](test-images/Tuggy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [('n02093428', 'American_Staffordshire_terrier', 0.7632004), ('n02093256', 'Staffordshire_bullterrier', 0.13892087), ('n02108422', 'bull_mastiff', 0.042401902)]\n"
     ]
    }
   ],
   "source": [
    "# load the image from file\n",
    "tuggy = load_img('test-images/Tuggy.png', target_size=(224, 224))\n",
    "# convert the image pixels to a numpy array\n",
    "tuggy = img_to_array(tuggy)\n",
    "# reshape data for the model\n",
    "tuggy = tuggy.reshape((1, tuggy.shape[0], tuggy.shape[1], tuggy.shape[2]))\n",
    "# prepare the image for the VGG model\n",
    "tuggy = preprocess_input(tuggy)\n",
    "# predict the probability across all output classes\n",
    "prob = model.predict(tuggy)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "print('Predicted:', decode_predictions(prob, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, I can see that the top 3 objects in the image are classified as: \n",
    "* \"American_Staffordshire_terrier\" with a 76.3% likelihood.\n",
    "* \"Staffordshire_bullterrier\" with a 13.9% likelihood.\n",
    "* \"bull_mastiff\" with a 4.2% likelihood.\n",
    "\n",
    "Here's another image from the video with top 5 animal stories:\n",
    "\n",
    "![farm](test-images/farm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [('n02782093', 'balloon', 0.96484685), ('n02692877', 'airship', 0.019180182), ('n03888257', 'parachute', 0.009387733)]\n"
     ]
    }
   ],
   "source": [
    "# load the image from file\n",
    "farm = load_img('test-images/farm.png', target_size=(224, 224))\n",
    "# convert the image pixels to a numpy array\n",
    "farm = img_to_array(farm)\n",
    "# reshape data for the model\n",
    "farm = farm.reshape((1, farm.shape[0], farm.shape[1], farm.shape[2]))\n",
    "# prepare the image for the VGG model\n",
    "farm = preprocess_input(farm)\n",
    "# predict the probability across all output classes\n",
    "prob = model.predict(farm)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "print('Predicted:', decode_predictions(prob, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, I can see that the top 3 objects in the image are classified as: \n",
    "* \"Balloon\" with a 96.5% likelihood.\n",
    "* \"Airship\" with a 1.9% likelihood.\n",
    "* \"Parachute\" with a 0.9% likelihood."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
